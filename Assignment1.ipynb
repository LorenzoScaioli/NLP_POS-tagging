{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zewt2U1V8Mhk",
        "outputId": "5f528126-7b08-481b-984d-ad0c8bd6e61d"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN3ZOypI8Mhr"
      },
      "source": [
        "Importants hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
        "\n",
        "**Keywords**: POS tagging, Sequence labelling, RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU8u6PyGerJ1"
      },
      "source": [
        "\n",
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "* Eleonora Mancini -> e.mancini@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn5lqLjaerJ2"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "You are tasked to address the task of POS tagging.\n",
        "\n",
        "<center>\n",
        "    <img src=\"images/pos_tagging.png\" alt=\"POS tagging\" />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl85WY79erJ2"
      },
      "source": [
        "# [Task 1 - 0.5 points] Corpus\n",
        "\n",
        "You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n",
        "\n",
        "**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n",
        "\n",
        "### Example\n",
        "\n",
        "```Pierre\tNNP\t2\n",
        "Vinken\tNNP\t8\n",
        ",\t,\t2\n",
        "61\tCD\t5\n",
        "years\tNNS\t6\n",
        "old\tJJ\t2\n",
        ",\t,\t2\n",
        "will\tMD\t0\n",
        "join\tVB\t8\n",
        "the\tDT\t11\n",
        "board\tNN\t9\n",
        "as\tIN\t9\n",
        "a\tDT\t15\n",
        "nonexecutive\tJJ\t15\n",
        "director\tNN\t12\n",
        "Nov.\tNNP\t9\n",
        "29\tCD\t16\n",
        ".\t.\t8\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DBSDHSeerJ3"
      },
      "source": [
        "### Splits\n",
        "\n",
        "The corpus contains 200 documents.\n",
        "\n",
        "   * **Train**: Documents 1-100\n",
        "   * **Validation**: Documents 101-150\n",
        "   * **Test**: Documents 151-199"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBhINLEaerJ3"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Download** the corpus.\n",
        "* **Encode** the corpus into a pandas.DataFrame object.\n",
        "* **Split** it in training, validation, and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azUjXF08erJ4"
      },
      "source": [
        "#### Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp4Q3iL6erJ4"
      },
      "source": [
        "#### Out of Vocabulary (OOV) words in training set\n",
        "We see words in the training set that are not alredy embedded through Glove (50) model, in addition we define the set oov_terms with all those words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:32.878447Z",
          "iopub.status.busy": "2023-11-20T17:28:32.878004Z",
          "iopub.status.idle": "2023-11-20T17:28:33.261992Z",
          "shell.execute_reply": "2023-11-20T17:28:33.261143Z",
          "shell.execute_reply.started": "2023-11-20T17:28:32.878414Z"
        },
        "id": "geRWC2XxerJ4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# file management\n",
        "import os\n",
        "import urllib\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# dataframe management\n",
        "import pandas as pd\n",
        "\n",
        "# data manipulation\n",
        "import numpy as np\n",
        "\n",
        "# for readability\n",
        "from typing import Iterable\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:34:18.348890Z",
          "iopub.status.busy": "2023-11-20T17:34:18.348168Z",
          "iopub.status.idle": "2023-11-20T17:34:35.583647Z",
          "shell.execute_reply": "2023-11-20T17:34:35.582521Z",
          "shell.execute_reply.started": "2023-11-20T17:34:18.348854Z"
        },
        "id": "GVhN0_EzerKS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ! pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yy-pyoDJ8Mhy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed: 1451601747\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import secrets\n",
        "\n",
        "def fix_random(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    generator = np.random.default_rng(seed)\n",
        "\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "    return generator\n",
        "\n",
        "seed = secrets.randbits(32)\n",
        "print(f\"Seed: {seed}\")\n",
        "rand_gen = fix_random(seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:33.264066Z",
          "iopub.status.busy": "2023-11-20T17:28:33.263670Z",
          "iopub.status.idle": "2023-11-20T17:28:33.273576Z",
          "shell.execute_reply": "2023-11-20T17:28:33.272666Z",
          "shell.execute_reply.started": "2023-11-20T17:28:33.264039Z"
        },
        "id": "5AHbyJvIerJ6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_url(download_path: Path, url: str):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
        "\n",
        "def download_dataset(download_path: Path, url: str):\n",
        "    print(\"Downloading dataset...\")\n",
        "    download_url(url=url, download_path=download_path)\n",
        "    print(\"Download complete!\")\n",
        "\n",
        "def extract_dataset(download_path: Path, extract_path: Path):\n",
        "    print(\"Extracting dataset... (it may take a while...)\")\n",
        "\n",
        "    with zipfile.ZipFile(download_path) as loaded_tar:\n",
        "        loaded_tar.extractall(path=extract_path, pwd=None)\n",
        "    print(\"Extraction completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:33.274952Z",
          "iopub.status.busy": "2023-11-20T17:28:33.274676Z",
          "iopub.status.idle": "2023-11-20T17:28:33.538381Z",
          "shell.execute_reply": "2023-11-20T17:28:33.537434Z",
          "shell.execute_reply.started": "2023-11-20T17:28:33.274927Z"
        },
        "id": "TjwV2cFTerJ7",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current work directory: c:\\Users\\Utente\\Desktop\\UNIVERSITA'\\AI\\2 Anno\\Natural Language Processing\\_ Esame\\Assignment 1\\NLP_POS-tagging\n"
          ]
        }
      ],
      "source": [
        "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "print(f\"Current work directory: {Path.cwd()}\")\n",
        "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
        "\n",
        "if not dataset_folder.exists():\n",
        "    dataset_folder.mkdir(parents=True)\n",
        "\n",
        "dataset_tar_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
        "dataset_path = dataset_folder.joinpath(dataset_name)\n",
        "\n",
        "if not dataset_tar_path.exists():\n",
        "    download_dataset(dataset_tar_path, url)\n",
        "\n",
        "if not dataset_path.exists():\n",
        "    extract_dataset(dataset_tar_path, dataset_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DVROHUherJ9"
      },
      "source": [
        "#### Encode and Split\n",
        "\n",
        "The aim of the code below is to find a way to create a dataframe starting from all the files downloaded before.\n",
        "For every downloaded file, we check the number through the function find_number(), we decide if it belongs to train, validation or test given that number, we then split it into rows to get the word and the POS and to check where a phrase ends. Given all this informations we can create a list whose columns are:\n",
        "1. num_file: the number of the file\n",
        "2. phrase_id: the id of the phrase contained in a file\n",
        "3. text: the text that has to be analyzed\n",
        "4. pos: the tag assigned to the text\n",
        "5. split: the split to which the text belongs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:33.540010Z",
          "iopub.status.busy": "2023-11-20T17:28:33.539685Z",
          "iopub.status.idle": "2023-11-20T17:28:33.545402Z",
          "shell.execute_reply": "2023-11-20T17:28:33.544379Z",
          "shell.execute_reply.started": "2023-11-20T17:28:33.539983Z"
        },
        "id": "OaiKoGb3erJ9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def find_number(string):\n",
        "    return re.findall(r'\\d+', string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:33.549435Z",
          "iopub.status.busy": "2023-11-20T17:28:33.548953Z",
          "iopub.status.idle": "2023-11-20T17:28:33.785708Z",
          "shell.execute_reply": "2023-11-20T17:28:33.784627Z",
          "shell.execute_reply.started": "2023-11-20T17:28:33.549399Z"
        },
        "id": "TTtotZNkerJ9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataframe_rows = []\n",
        "id = 0\n",
        "\n",
        "folder = dataset_folder.joinpath(dataset_name)\n",
        "for file_path in folder.glob('*.dp'):\n",
        "    num_file = int(find_number(file_path.name)[0])\n",
        "    id = 1\n",
        "\n",
        "    with file_path.open(mode='r', encoding='utf-8') as text_file:\n",
        "\n",
        "        if num_file < 101:\n",
        "            split = \"train\"\n",
        "        elif num_file >= 101 and num_file < 151:\n",
        "            split = \"validation\"\n",
        "        else:\n",
        "            split = \"test\"\n",
        "\n",
        "        for row in text_file.readlines():\n",
        "            if row=='\\n' or row=='':\n",
        "                id += 1\n",
        "\n",
        "            else:\n",
        "                text, pos, _ = row.split('\\t')\n",
        "\n",
        "                dataframe_row = {\n",
        "                    \"num_file\": num_file,\n",
        "                    \"phrase_id\": str(num_file) + \"_\" + str(id),\n",
        "                    \"text\": text,\n",
        "                    \"pos\": pos,\n",
        "                    \"split\": split\n",
        "                }\n",
        "\n",
        "                dataframe_rows.append(dataframe_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:33.787673Z",
          "iopub.status.busy": "2023-11-20T17:28:33.787248Z",
          "iopub.status.idle": "2023-11-20T17:28:34.014786Z",
          "shell.execute_reply": "2023-11-20T17:28:34.013688Z",
          "shell.execute_reply.started": "2023-11-20T17:28:33.787633Z"
        },
        "id": "O01Zu0lAerJ-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(dataframe_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56t_8upaerJ-"
      },
      "source": [
        "# [Task 2 - 0.5 points] Text encoding\n",
        "\n",
        "To train a neural POS tagger, you first need to encode text into numerical format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IpIpTPoerJ_"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Embed words using **GloVe embeddings**.\n",
        "* You are **free** to pick any embedding dimension.\n",
        "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:34.016375Z",
          "iopub.status.busy": "2023-11-20T17:28:34.016029Z",
          "iopub.status.idle": "2023-11-20T17:28:34.021311Z",
          "shell.execute_reply": "2023-11-20T17:28:34.020216Z",
          "shell.execute_reply.started": "2023-11-20T17:28:34.016345Z"
        },
        "id": "hpVTQ-o6erJ_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# typing\n",
        "from typing import List, Callable, Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sPpHwhHerJ_"
      },
      "source": [
        "#### Text pre-processing\n",
        "In the code below we pre-processed the df dataframe in order to reduce the number of different words. Our text pre-processing consist just in lowering the text of words. <br>\n",
        "**NB: should we add somenthing to the pre processing?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:34.023641Z",
          "iopub.status.busy": "2023-11-20T17:28:34.023094Z",
          "iopub.status.idle": "2023-11-20T17:28:35.756398Z",
          "shell.execute_reply": "2023-11-20T17:28:35.755525Z",
          "shell.execute_reply.started": "2023-11-20T17:28:34.023553Z"
        },
        "id": "_BBGJLkeerJ_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:35.757918Z",
          "iopub.status.busy": "2023-11-20T17:28:35.757604Z",
          "iopub.status.idle": "2023-11-20T17:28:35.762885Z",
          "shell.execute_reply": "2023-11-20T17:28:35.761863Z",
          "shell.execute_reply.started": "2023-11-20T17:28:35.757892Z"
        },
        "id": "59cctQmxerKA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def lower(text: str) -> str:\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:35.764417Z",
          "iopub.status.busy": "2023-11-20T17:28:35.764075Z",
          "iopub.status.idle": "2023-11-20T17:28:35.775830Z",
          "shell.execute_reply": "2023-11-20T17:28:35.774984Z",
          "shell.execute_reply.started": "2023-11-20T17:28:35.764366Z"
        },
        "id": "jSzHoq7BerKA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "PREPROCESSING_PIPELINE = [\n",
        "                          lower\n",
        "                          ]\n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    filter_methods = filter_methods or PREPROCESSING_PIPELINE\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:35.777282Z",
          "iopub.status.busy": "2023-11-20T17:28:35.776990Z",
          "iopub.status.idle": "2023-11-20T17:28:35.903065Z",
          "shell.execute_reply": "2023-11-20T17:28:35.902113Z",
          "shell.execute_reply.started": "2023-11-20T17:28:35.777257Z"
        },
        "id": "MQi5fld5erKA",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-processing completed!\n"
          ]
        }
      ],
      "source": [
        "df['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n",
        "print(\"Pre-processing completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h47lVPfierKA"
      },
      "source": [
        "**Vocabulary creation for training set** <br>\n",
        "We define a vocabulary for the training set assigning to each word a random index, the building_vocabulary function returns a list containing:<br>\n",
        "- word vocabulary: vocabulary index to word\n",
        "- inverse word vocabulary: word to vocabulary index\n",
        "- word listing: set of unique terms that build up the vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:35.905088Z",
          "iopub.status.busy": "2023-11-20T17:28:35.904709Z",
          "iopub.status.idle": "2023-11-20T17:28:35.978078Z",
          "shell.execute_reply": "2023-11-20T17:28:35.977203Z",
          "shell.execute_reply.started": "2023-11-20T17:28:35.905052Z"
        },
        "id": "1sR32eMrerKB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_train = df[df['split']=='train']\n",
        "df_val = df[df['split']=='validation']\n",
        "df_test = df[df['split']=='test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc3eQ6AderKB"
      },
      "source": [
        "**GloVe embeddings (50)** <br>\n",
        "Download GloVe 50 embedding where most of the words are alredy embedded in an embedding model that associate each word to a vector of dimension 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:35.988166Z",
          "iopub.status.busy": "2023-11-20T17:28:35.987892Z",
          "iopub.status.idle": "2023-11-20T17:28:44.567366Z",
          "shell.execute_reply": "2023-11-20T17:28:44.566483Z",
          "shell.execute_reply.started": "2023-11-20T17:28:35.988142Z"
        },
        "id": "stvoRolverKB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install gensim\n",
        "import gensim\n",
        "from gensim import downloader as gloader\n",
        "\n",
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    download_path = \"\"\n",
        "\n",
        "    if model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available one: glove\")\n",
        "\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:29:17.986972Z",
          "iopub.status.busy": "2023-11-20T17:29:17.986587Z",
          "iopub.status.idle": "2023-11-20T17:29:17.992578Z",
          "shell.execute_reply": "2023-11-20T17:29:17.991698Z",
          "shell.execute_reply.started": "2023-11-20T17:29:17.986945Z"
        },
        "id": "D67KXftCerKC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                    word_listing: List[str]):\n",
        "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:28:44.568939Z",
          "iopub.status.busy": "2023-11-20T17:28:44.568460Z",
          "iopub.status.idle": "2023-11-20T17:29:17.221564Z",
          "shell.execute_reply": "2023-11-20T17:29:17.220762Z",
          "shell.execute_reply.started": "2023-11-20T17:28:44.568913Z"
        },
        "id": "HRmCZlDYerKC",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (400001, 50)\n",
            "Total OOV terms: 359 (4.85%)\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 50\n",
        "embedding_model = load_embedding_model(model_type=\"glove\",\n",
        "                                    embedding_dimension=embedding_dim)\n",
        "\n",
        "vocab = {} # word to idx\n",
        "embedding_matrix_glove = np.zeros((400001, embedding_dim))\n",
        "\n",
        "for i in range(0, 400000):\n",
        "    vocab[embedding_model.index_to_key[i]] = i+1\n",
        "    embedding_matrix_glove[i+1] = embedding_model.vectors[i]\n",
        "\n",
        "print(f'Embedding matrix shape: {embedding_matrix_glove.shape}')\n",
        "\n",
        "word_listing = set(df_train['text'])\n",
        "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
        "oov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\n",
        "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1zIe3RWerKD"
      },
      "source": [
        "Here we add the OOV in the train set to the vocabulary and the embedded matrix </br>\n",
        "*NB: maybe is better to define the OOV as the mean of all the other word, link for report (https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt)* <br>\n",
        "Our embedding matrix has the following columns:\n",
        "- column 0 is all zeros, represents the embedding vector for padding\n",
        "- columns 1 to 400001 are the embedding vectors for the words in GloVe\n",
        "- columns 400002 to 400360 are the embedding vectors for the words OOV in the training set (random vector)\n",
        "- column 400361 is the embedding vector for the words UNK in the final vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:29:18.083837Z",
          "iopub.status.busy": "2023-11-20T17:29:18.083544Z",
          "iopub.status.idle": "2023-11-20T17:29:38.116881Z",
          "shell.execute_reply": "2023-11-20T17:29:38.115940Z",
          "shell.execute_reply.started": "2023-11-20T17:29:18.083804Z"
        },
        "id": "3YIEwC86erKD",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New embedding matrix size: (400361, 50)\n"
          ]
        }
      ],
      "source": [
        "for word in oov_terms:\n",
        "    vocab[word] = 400002 + oov_terms.index(word)\n",
        "    embedding_matrix_glove = np.append(embedding_matrix_glove, rand_gen.uniform(-0.25, 0.25, 50).reshape(1, 50), axis=0)\n",
        "\n",
        "vocab['[UNK]'] = len(vocab) + 1\n",
        "average_oov = np.mean(embedding_matrix_glove, axis=0)\n",
        "embedding_matrix = np.append(embedding_matrix_glove, average_oov.reshape(1, 50), axis=0)\n",
        "\n",
        "vocab['[PAD]'] = 0\n",
        "\n",
        "print(f\"New embedding matrix size: {embedding_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_5YoZOserKE"
      },
      "source": [
        "#### Embedding for training set\n",
        "We create the embedding matrix for all the training set:\n",
        "- using GloVe embeddings for alredy known words\n",
        "- assigning to each OOV word a random value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:33:55.859861Z",
          "iopub.status.busy": "2023-11-20T17:33:55.858959Z",
          "iopub.status.idle": "2023-11-20T17:33:55.874207Z",
          "shell.execute_reply": "2023-11-20T17:33:55.873349Z",
          "shell.execute_reply.started": "2023-11-20T17:33:55.859823Z"
        },
        "id": "-zKpiu-4erKE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# utils functions for padding and encoding\n",
        "\n",
        "list_of_pos = list(set(df_train['pos']))\n",
        "\n",
        "def pos_to_int(string):\n",
        "    if string == '[PAD]':\n",
        "        return np.zeros((45,), dtype=int).tolist()\n",
        "    length = len(list_of_pos)\n",
        "    for i in range(length):\n",
        "        if list_of_pos[i] == string:\n",
        "            return [1 if j == i else 0 for j in range(length)]\n",
        "\n",
        "def int_to_pos(phrase_hot_encodings):\n",
        "    return [list_of_pos[np.argmax(word)] for word in phrase_hot_encodings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:39:52.402377Z",
          "iopub.status.busy": "2023-11-20T17:39:52.401596Z",
          "iopub.status.idle": "2023-11-20T17:39:52.407102Z",
          "shell.execute_reply": "2023-11-20T17:39:52.406241Z",
          "shell.execute_reply.started": "2023-11-20T17:39:52.402342Z"
        },
        "id": "H1-H7HKierKV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def assign_idx(word):\n",
        "    try:\n",
        "        idx = vocab[word]\n",
        "    except(KeyError):\n",
        "        idx = vocab['[UNK]']\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T18:49:24.639966Z",
          "iopub.status.busy": "2023-11-20T18:49:24.639562Z",
          "iopub.status.idle": "2023-11-20T18:50:13.110727Z",
          "shell.execute_reply": "2023-11-20T18:50:13.109704Z",
          "shell.execute_reply.started": "2023-11-20T18:49:24.639934Z"
        },
        "id": "s7cq2w3werKV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "x_train = [ [assign_idx(word) for word in df_train[df_train['phrase_id']==nr_phrase]['text']] for nr_phrase in set(df_train['phrase_id']) ]\n",
        "x_val   = [ [assign_idx(word) for word in df_val[df_val['phrase_id']==nr_phrase]['text']] for nr_phrase in set(df_val['phrase_id']) ]\n",
        "x_test   = [ [assign_idx(word) for word in df_test[df_test['phrase_id']==nr_phrase]['text']] for nr_phrase in set(df_test['phrase_id']) ]\n",
        "\n",
        "y_train = [ [pos_to_int(pos) for pos in df_train[df_train['phrase_id']==nr_phrase]['pos']] for nr_phrase in set(df_train['phrase_id']) ]\n",
        "y_val   = [ [pos_to_int(pos) for pos in df_val[df_val['phrase_id']==nr_phrase]['pos']] for nr_phrase in set(df_val['phrase_id']) ]\n",
        "y_test   = [ [pos_to_int(pos) for pos in df_test[df_test['phrase_id']==nr_phrase]['pos']] for nr_phrase in set(df_test['phrase_id']) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T18:50:13.112888Z",
          "iopub.status.busy": "2023-11-20T18:50:13.112544Z",
          "iopub.status.idle": "2023-11-20T18:50:21.752690Z",
          "shell.execute_reply": "2023-11-20T18:50:21.751864Z",
          "shell.execute_reply.started": "2023-11-20T18:50:13.112859Z"
        },
        "id": "oLAKfqJ0erKW",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of all phrases after padding will be  249\n"
          ]
        }
      ],
      "source": [
        "#In order to put the data data in data in the model we need to pad the array representing the words / pos\n",
        "\n",
        "pad = max(max([len(phrase) for phrase in x_train]), max([len(phrase) for phrase in x_val]), max([len(phrase) for phrase in x_test]))\n",
        "print(\"The length of all phrases after padding will be \", pad)\n",
        "\n",
        "x_train_pad = [phrase + np.zeros((pad-len(phrase),), dtype=int).tolist() for phrase in x_train]\n",
        "x_val_pad = [phrase + np.zeros((pad-len(phrase),), dtype=int).tolist() for phrase in x_val]\n",
        "x_test_pad = [phrase + np.zeros((pad-len(phrase),), dtype=int).tolist() for phrase in x_test]\n",
        "\n",
        "y_train_pad = [phrase + np.zeros((pad-len(phrase), 45)).tolist() for phrase in y_train]\n",
        "y_val_pad = [phrase + np.zeros((pad-len(phrase), 45)).tolist() for phrase in y_val]\n",
        "y_test_pad = [phrase + np.zeros((pad-len(phrase), 45)).tolist() for phrase in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "o2A26JuUm8Sz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "he said chrysler fully expects to have them installed across its light-truck line by the sept. 1 , 1991 , deadline . [PAD]\n",
            "['PRP', 'VBD', 'NNP', 'RB', 'VBZ', 'TO', 'VB', 'PRP', 'VBN', 'IN', 'PRP$', 'JJ', 'NN', 'IN', 'DT', 'NNP', 'CD', ',', 'CD', ',', 'NN', '.']\n"
          ]
        }
      ],
      "source": [
        "# how is represented our data\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in vocab.items()])\n",
        "def decode_phrase(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "print(decode_phrase(x_train_pad[0][:len(x_train[0])+1]))\n",
        "print(int_to_pos(y_train_pad[0][:len(x_train[0])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WCaJihaa8Mh6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['#', 'RBR', 'LS', 'WRB', 'IN', '.', 'RBS', '-LRB-', 'TO', '-RRB-', 'WP$', 'UH', '``', 'FW', 'WDT', ',', 'JJS', 'RP', 'CC', 'NNS', 'SYM', 'VBD', 'MD', 'VBG', 'EX', 'CD', 'VB', 'JJ', 'NNP', 'WP', ':', 'VBN', \"''\", 'PRP', 'JJR', 'PDT', 'VBP', 'NN', 'POS', 'DT', 'VBZ', 'PRP$', '$', 'RB', 'NNPS']\n"
          ]
        }
      ],
      "source": [
        "print(list_of_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDJl1QCmerKE"
      },
      "source": [
        "# [Task 3 - 1.0 points] Model definition\n",
        "\n",
        "You are now tasked to define your neural POS tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx-atMXterKE"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
        "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
        "\n",
        "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
        "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
        "\n",
        "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
        "\n",
        "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pc8WXOoerKS"
      },
      "source": [
        "#### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:36:38.089888Z",
          "iopub.status.busy": "2023-11-20T17:36:38.088830Z",
          "iopub.status.idle": "2023-11-20T17:36:38.094485Z",
          "shell.execute_reply": "2023-11-20T17:36:38.093644Z",
          "shell.execute_reply.started": "2023-11-20T17:36:38.089851Z"
        },
        "id": "WtAw13JjerKT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "GloVe_dim = 50 # GloVe embedding\n",
        "units_bi = 64\n",
        "units_dense = 128\n",
        "\n",
        "n_unique_words = len(vocab) # input dimension\n",
        "outputs_dim = len(list_of_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:36:46.505878Z",
          "iopub.status.busy": "2023-11-20T17:36:46.505146Z",
          "iopub.status.idle": "2023-11-20T17:36:52.733684Z",
          "shell.execute_reply": "2023-11-20T17:36:52.732727Z",
          "shell.execute_reply.started": "2023-11-20T17:36:46.505845Z"
        },
        "id": "hyw7dSlVerKT",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 50)          20018050  \n",
            "                                                                 \n",
            " masking (Masking)           (None, None, 50)          0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, None, 128)         58880     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, None, 45)          5805      \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20082735 (76.61 MB)\n",
            "Trainable params: 64685 (252.68 KB)\n",
            "Non-trainable params: 20018050 (76.36 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "baseline = tf.keras.Sequential(name='Baseline')\n",
        "\n",
        "# baseline.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=True, trainable=False))\n",
        "baseline.add(layers.Embedding(n_unique_words, GloVe_dim, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
        "baseline.add(layers.Masking(mask_value=0))\n",
        "baseline.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
        "baseline.add(layers.TimeDistributed(layers.Dense(outputs_dim, activation='softmax')))\n",
        "\n",
        "baseline.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp2BrTbderKT"
      },
      "source": [
        "#### Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:37:03.349784Z",
          "iopub.status.busy": "2023-11-20T17:37:03.349398Z",
          "iopub.status.idle": "2023-11-20T17:37:03.973614Z",
          "shell.execute_reply": "2023-11-20T17:37:03.972655Z",
          "shell.execute_reply.started": "2023-11-20T17:37:03.349753Z"
        },
        "id": "CBDSLT6MerKT",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 50)          20018050  \n",
            "                                                                 \n",
            " masking_1 (Masking)         (None, None, 50)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, None, 128)         58880     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, None, 128)         98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, None, 45)          5805      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20181551 (76.99 MB)\n",
            "Trainable params: 163501 (638.68 KB)\n",
            "Non-trainable params: 20018050 (76.36 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_1 = tf.keras.Sequential(name='Model_1')\n",
        "\n",
        "# model_1.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=True, trainable=False))\n",
        "model_1.add(layers.Embedding(n_unique_words, GloVe_dim, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
        "model_1.add(layers.Masking(mask_value=0))\n",
        "model_1.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
        "model_1.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
        "model_1.add(layers.TimeDistributed(layers.Dense(outputs_dim, activation='softmax')))\n",
        "\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PX07mrgerKU"
      },
      "source": [
        "#### Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-20T17:38:26.347478Z",
          "iopub.status.busy": "2023-11-20T17:38:26.346994Z",
          "iopub.status.idle": "2023-11-20T17:38:26.793587Z",
          "shell.execute_reply": "2023-11-20T17:38:26.792635Z",
          "shell.execute_reply.started": "2023-11-20T17:38:26.347442Z"
        },
        "id": "bVOOuVFOerKU",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 50)          20018050  \n",
            "                                                                 \n",
            " masking_2 (Masking)         (None, None, 50)          0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, None, 128)         58880     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, None, 128)         16512     \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, None, 45)          5805      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20099247 (76.67 MB)\n",
            "Trainable params: 81197 (317.18 KB)\n",
            "Non-trainable params: 20018050 (76.36 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_2 = tf.keras.Sequential(name='Model_2')\n",
        "\n",
        "# model_2.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=True, trainable=False))\n",
        "model_2.add(layers.Embedding(n_unique_words, GloVe_dim, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
        "model_2.add(layers.Masking(mask_value=0))\n",
        "model_2.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
        "model_2.add(layers.Dense(units_dense, activation='relu'))\n",
        "model_2.add(layers.TimeDistributed(layers.Dense(outputs_dim, activation='softmax')))\n",
        "\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cQah6XJerKW"
      },
      "source": [
        "# [Task 4 - 1.0 points] Metrics\n",
        "\n",
        "Before training the models, you are tasked to define the evaluation metrics for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELGz0EB5erKW"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
        "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n",
        "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qvrXuZVerKW"
      },
      "source": [
        "**Note**: What about OOV tokens?\n",
        "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
        "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
        "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9rqWJlb_dGxN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "ignore_classes = ['[PAD]', '#', '``', '-RRB-', \"''\", '$', 'SYM', ':', '.', ',', '-LRB-']\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, x, y_true):\n",
        "        super().__init__()\n",
        "        self.x = x\n",
        "        self.y_true = y_true\n",
        "        self.tags = [pos for pos in list_of_pos if pos not in ignore_classes]\n",
        "        self.metrics_dict = {'macro_f1': {label: [] for label in self.tags},\n",
        "                             'precision': {label: [] for label in self.tags},\n",
        "                             'recall': {label: [] for label in self.tags},\n",
        "                             'accuracy': {label: [] for label in self.tags}}\n",
        "        self.cm = None\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred = self.model.predict(self.x)\n",
        "        y_true_pos = [int_to_pos(true) for true in self.y_true]\n",
        "        y_pred_pos = [int_to_pos(pred) for pred in y_pred]\n",
        "\n",
        "        y_true_pos_flat = [tag for phrase in y_true_pos for tag in phrase]\n",
        "        y_pred_pos_flat = [tag for phrase in y_pred_pos for tag in phrase]\n",
        "\n",
        "        y_true_pos_clean = []\n",
        "        y_pred_pos_clean = []\n",
        "        for i, tag in enumerate(y_true_pos_flat):\n",
        "            if tag not in ignore_classes:\n",
        "                y_true_pos_clean.append(tag)\n",
        "                y_pred_pos_clean.append(y_pred_pos_flat[i])\n",
        "\n",
        "        # util prints for debug after each epoch\n",
        "\n",
        "        # p = np.random.randint(0, len(y_true_pos))\n",
        "        # len_of_phrase = np.count_nonzero(self.x[p])\n",
        "        # print(f\"Random phrase: {decode_phrase(self.x[p][0:len_of_phrase])}\")\n",
        "        # print(f\"True POS     : {y_true_pos[p][0:len_of_phrase]}\")\n",
        "        # print(f\"Predicted POS: {y_pred_pos[p][0:len_of_phrase]}\")\n",
        "\n",
        "        self.cm = confusion_matrix(y_true_pos_clean, y_pred_pos_clean, labels=self.tags)\n",
        "        precision, recall, f1, accuracy = np.zeros(len(self.tags)), np.zeros(len(self.tags)), np.zeros(len(self.tags)), np.zeros(len(self.tags))\n",
        "        TP, FP, FN = np.zeros(len(self.tags)), np.zeros(len(self.tags)), np.zeros(len(self.tags))\n",
        "\n",
        "        for idx, label in enumerate(self.tags):\n",
        "            TP[idx] = self.cm[idx][idx]\n",
        "            FP[idx] = sum([self.cm[i][idx] for i in range(len(self.tags))]) - TP[idx]\n",
        "            FN[idx] = sum([self.cm[idx][i] for i in range(len(self.tags))]) - TP[idx]\n",
        "            precision[idx] = (TP[idx] / (TP[idx] + FP[idx])) if (TP[idx] + FP[idx]) > 0 else 0\n",
        "            recall[idx] = (TP[idx] / (TP[idx] + FN[idx])) if (TP[idx] + FN[idx]) > 0 else 0\n",
        "            f1[idx] = (2 * precision[idx] * recall[idx] / (precision[idx] + recall[idx])) if (precision[idx] + recall[idx]) > 0 else 0\n",
        "            mask = np.array(y_true_pos_clean) == label\n",
        "            masked_true = np.array(y_true_pos_clean)[mask]\n",
        "            masked_pred = np.array(y_pred_pos_clean)[mask]\n",
        "            accuracy[idx] = accuracy_score(masked_true, masked_pred)\n",
        "\n",
        "        for idx, label in enumerate(self.tags):\n",
        "            self.metrics_dict['precision'][label].append(precision[idx])\n",
        "            self.metrics_dict['recall'][label].append(recall[idx])\n",
        "            self.metrics_dict['macro_f1'][label].append(f1[idx])\n",
        "            self.metrics_dict['accuracy'][label].append(accuracy[idx])\n",
        "\n",
        "        precision = np.mean(precision)\n",
        "        recall = np.mean(recall)\n",
        "        f1 = np.mean(f1)\n",
        "        accuracy = np.mean(accuracy)\n",
        "        print(\"Metrics on val data ignoring sym, punct and pad:\")\n",
        "        print(f\"Macro F1-Score: {f1:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    def get_metrics_dict(self):\n",
        "        return self.metrics_dict\n",
        "\n",
        "    def get_cm(self):\n",
        "        return self.cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGZS-c3TerKX"
      },
      "source": [
        "# [Task 5 - 1.0 points] Training and Evaluation\n",
        "\n",
        "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS4QezxderKX"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Train **all** models on the train set.\n",
        "* Evaluate **all** models on the validation set.\n",
        "* Compute metrics on the validation set.\n",
        "* Pick **at least** three seeds for robust estimation.\n",
        "* Pick the **best** performing model according to the observed validation set performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_model(model_to_download, model_seed=0):\n",
        "    models_folder = Path.cwd().joinpath(\"models\")\n",
        "    \n",
        "    if not models_folder.exists():\n",
        "        models_folder.mkdir(parents=True)\n",
        "        \n",
        "    model_folder = models_folder.joinpath(f\"{model_to_download.name}\")\n",
        "\n",
        "    if not model_folder.exists():\n",
        "        model_folder.mkdir(parents=True)\n",
        "    \n",
        "    for ckpt_path in [\"my_cm.npy\", \"my_history.npy\", \"my_metrics.npy\"]: # [\"checkpoint\", \"cp.ckpt.data-00000-of-00001\", \"cp.ckpt.index\", \"my_cm.npy\", \"my_history.npy\", \"my_metrics.npy\"]:\n",
        "        url = f\"https://github.com/LorenzoScaioli/NLP-Models/raw/main/models_Assigment_1/{model_to_download.name}/{ckpt_path}\"\n",
        "\n",
        "        checkpoint_history = model_folder.joinpath(f\"{ckpt_path}\")\n",
        "\n",
        "        if not checkpoint_history.exists():\n",
        "            download_dataset(checkpoint_history, url)\n",
        "    \n",
        "    url = f\"https://github.com/LorenzoScaioli/NLP-Models/raw/main/models_Assigment_1/{model_to_download.name}/{model_to_download.name}_{model_seed}.h5\"\n",
        "    model_path = model_folder.joinpath(f\"{model_to_download.name}_{model_seed}.h5\")\n",
        "\n",
        "    if not model_path.exists():\n",
        "        download_dataset(model_path, url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ObgplNOt8Mh8"
      },
      "outputs": [],
      "source": [
        "def get_model(model, model_seed, train_model, batch_size=None, epochs=None):\n",
        "    model_path = Path.cwd().joinpath(f\"models/{model.name}/{model.name}_{model_seed}.h5\")\n",
        "    checkpoint_dir = Path.cwd().joinpath(f\"models/{model.name}\")\n",
        "    checkpoint_path = Path.cwd().joinpath(f\"models/{model.name}/cp.ckpt\")\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
        "                optimizer=tf.keras.optimizers.experimental.Adadelta(learning_rate=1))\n",
        "\n",
        "    if train_model:\n",
        "        checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True)\n",
        "        early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5, factor=0.2, min_lr=0.001)\n",
        "        metrics_callback = MetricsCallback(x_val_pad, y_val_pad)\n",
        "        \n",
        "        history = model.fit(x_train_pad, y_train_pad, batch_size=batch_size, epochs=epochs,\n",
        "                            validation_data=(x_val_pad, y_val_pad), verbose=1,\n",
        "                            callbacks=[metrics_callback, early_stopping, reduce_lr, checkpoint_cb])\n",
        "        # model.save_weights(checkpoint_path)\n",
        "        model.save(model_path)\n",
        "\n",
        "        history = history.history\n",
        "        np.save(checkpoint_dir.joinpath('my_history.npy'), history)\n",
        "        metrics_dict = metrics_callback.get_metrics_dict()\n",
        "        np.save(checkpoint_dir.joinpath('my_metrics.npy'), metrics_dict)\n",
        "        cm = metrics_callback.get_cm()\n",
        "        np.save(checkpoint_dir.joinpath('my_cm.npy'), cm)\n",
        "    else:\n",
        "        # model.load_weights(checkpoint_path, by_name=True, skip_mismatch=True)\n",
        "        download_model(model, model_seed)\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "        history = np.load(checkpoint_dir.joinpath('my_history.npy'), allow_pickle='TRUE').item()\n",
        "        metrics_dict = np.load(checkpoint_dir.joinpath('my_metrics.npy'), allow_pickle='TRUE').item()\n",
        "        cm = np.load(checkpoint_dir.joinpath('my_cm.npy'), allow_pickle='TRUE')\n",
        "    \n",
        "    return model, history, metrics_dict, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HF42CZqRh6Ij"
      },
      "outputs": [],
      "source": [
        "model = baseline\n",
        "model_seed = 0\n",
        "train_model = False\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "model, history, metrics_dict, cm = get_model(model, model_seed, train_model, batch_size, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [baseline, model_1, model_2]\n",
        "for model in models:\n",
        "    model, history, metrics_dict, cm = get_model(model, model_seed, train_model, batch_size, epochs)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECtlUCvx8Mh8"
      },
      "outputs": [],
      "source": [
        "# if train_model:\n",
        "#     history = history.history\n",
        "#     np.save(f'drive/MyDrive/NLPmodels/{model.name}/my_history.npy', history)\n",
        "#     metrics_dict = metrics_callback.get_metrics_dict()\n",
        "#     np.save(f'drive/MyDrive/NLPmodels/{model.name}/my_metrics.npy', metrics_dict)\n",
        "#     cm = metrics_callback.get_cm()\n",
        "#     np.save(f'drive/MyDrive/NLPmodels/{model.name}/my_cm.npy', cm)\n",
        "# else:\n",
        "#     history = np.load(f'drive/MyDrive/NLPmodels/{model.name}/my_history.npy', allow_pickle='TRUE').item()\n",
        "#     metrics_dict = np.load(f'drive/MyDrive/NLPmodels/{model.name}/my_metrics.npy', allow_pickle='TRUE').item()\n",
        "#     cm = np.load(f'drive/MyDrive/NLPmodels/{model.name}/my_cm.npy', allow_pickle='TRUE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kLdlaZXZcn1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def metrics_plots(metrics_dict, model_name):\n",
        "    keys = list(metrics_dict.keys())\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title('Metrics - ' + model_name)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Metrics\")\n",
        "    for metric in keys:\n",
        "        if metric != 'accuracy':\n",
        "            mean_metric = np.mean(list(metrics_dict[metric].values()), axis=0)\n",
        "            plt.plot(mean_metric, label=metric)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Accuracy - ' + model_name)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    mean_accuracy = np.mean(list(metrics_dict['accuracy'].values()), axis=0)\n",
        "    plt.plot(mean_accuracy, label='accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "metrics_plots(metrics_dict, model.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ2JXbCp8Mh8"
      },
      "outputs": [],
      "source": [
        "y_train_pos = [ [pos for pos in df_val[df_val['phrase_id']==nr_phrase]['pos']] for nr_phrase in set(df_val['phrase_id']) ]\n",
        "y_train_pos_flat = [tag for phrase in y_train_pos for tag in phrase]\n",
        "y_train_pos_clean = [tag for tag in y_train_pos_flat if tag not in ignore_classes]\n",
        "mf_tags = sorted(set(y_train_pos_clean), key=y_train_pos_clean.count, reverse=True)[:10]\n",
        "lf_tags = sorted(set(y_train_pos_clean), key=y_train_pos_clean.count)[:10]\n",
        "\n",
        "mf_metrics_dict = {'macro_f1': {tag: metrics_dict['macro_f1'][tag] for tag in mf_tags},\n",
        "                   'precision': {tag: metrics_dict['precision'][tag] for tag in mf_tags},\n",
        "                   'recall': {tag: metrics_dict['recall'][tag] for tag in mf_tags},\n",
        "                   'accuracy': {tag: metrics_dict['accuracy'][tag] for tag in mf_tags}}\n",
        "\n",
        "lf_metrics_dict = {'macro_f1': {tag: metrics_dict['macro_f1'][tag] for tag in lf_tags},\n",
        "                   'precision': {tag: metrics_dict['precision'][tag] for tag in lf_tags},\n",
        "                   'recall': {tag: metrics_dict['recall'][tag] for tag in lf_tags},\n",
        "                   'accuracy': {tag: metrics_dict['accuracy'][tag] for tag in lf_tags}}\n",
        "\n",
        "print(\"Most frequent tags: \", [(tag, y_train_pos_clean.count(tag)) for tag in mf_tags])\n",
        "print(\"Less frequent tags: \", [(tag, y_train_pos_clean.count(tag)) for tag in lf_tags])\n",
        "\n",
        "metrics_plots(mf_metrics_dict, model.name + \" - Most Frequent Tags\")\n",
        "metrics_plots(lf_metrics_dict, model.name + \" - Less Frequent Tags\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbR9p44W8Mh9"
      },
      "outputs": [],
      "source": [
        "def graph_plots(history, model_name):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['accuracy'])\n",
        "    plt.plot(history['val_accuracy'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(['accuracy', 'val_accuracy'])\n",
        "    plt.title('Accuracy - ' + model_name)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['loss', 'val_loss'])\n",
        "    plt.title('Loss - ' + model_name)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "graph_plots(history, model.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlUfuyig8Mh9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "pos_tags = [pos for pos in list_of_pos if pos not in ignore_classes]\n",
        "\n",
        "pos_tags_sorted = sorted(set(y_train_pos_clean), key=y_train_pos_clean.count, reverse=True)\n",
        "cm_sorted = cm[[pos_tags.index(tag) for tag in pos_tags_sorted]][:, [pos_tags.index(tag) for tag in pos_tags_sorted]]\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(cm_sorted, annot=True, fmt='d', xticklabels=pos_tags_sorted, yticklabels=pos_tags_sorted)\n",
        "plt.title('Confusion Matrix - ' + model.name)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = download_model(model=baseline, model_seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldnDXMJberKY"
      },
      "source": [
        "# [Task 6 - 1.0 points] Error Analysis\n",
        "\n",
        "You are tasked to evaluate your best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkJt90bTerKY"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Compare the errors made on the validation and test sets.\n",
        "* Aggregate model errors into categories (if possible)\n",
        "* Comment the about errors and propose possible solutions on how to address them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_scores = best_model.evaluate(x_test_pad, y_test_pad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = model.predict(x_test_pad)\n",
        "p = rand_gen.integers(0, len(x_test))\n",
        "len_of_phrase = len(x_test[p])\n",
        "print(f\"Random phrase: {decode_phrase(x_test[p])}\")\n",
        "print(f\"True POS     : {int_to_pos(y_test[p])}\")\n",
        "print(f\"Predicted POS: {int_to_pos(pred[p][0:len_of_phrase])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDyrD2lkerKY"
      },
      "source": [
        "# [Task 7 - 1.0 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzXPl4a8erKY"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwgIk9cMerKY"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbpGyQLP8Mh-"
      },
      "source": [
        "LaTex link https://it.overleaf.com/project/655cd1d053810bc1cd65fae8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soQhVYWUerKY"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
        "* You can upload **model weights** in a cloud repository and report the link in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gvMpihcerKY"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U_Mw939erKY"
      },
      "source": [
        "### Trainable Embeddings\n",
        "\n",
        "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XipBlZuWerKY"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "You **should not** change the architecture of a model (i.e., its layers).\n",
        "\n",
        "However, you are **free** to play with their hyper-parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxly3ITzerKZ"
      },
      "source": [
        "### Neural Libraries\n",
        "\n",
        "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ekET0dqerKZ"
      },
      "source": [
        "### Keras TimeDistributed Dense layer\n",
        "\n",
        "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skoUzF-LerKZ"
      },
      "source": [
        "### Error Analysis\n",
        "\n",
        "Some topics for discussion include:\n",
        "   * Model performance on most/less frequent classes.\n",
        "   * Precision/Recall curves.\n",
        "   * Confusion matrices.\n",
        "   * Specific misclassified samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVnkFOYverKZ"
      },
      "source": [
        "### Punctuation\n",
        "\n",
        "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
        "\n",
        "You should **ignore** it during metrics computation.\n",
        "\n",
        "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FVm4juGerKZ"
      },
      "source": [
        "# The End"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30588,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
