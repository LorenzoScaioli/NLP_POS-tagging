{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: POS tagging, Sequence labelling, RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "You are tasked to address the task of POS tagging.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/pos_tagging.png\" alt=\"POS tagging\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 1 - 0.5 points] Corpus\n",
    "\n",
    "You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n",
    "\n",
    "**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n",
    "\n",
    "### Example\n",
    "\n",
    "```Pierre\tNNP\t2\n",
    "Vinken\tNNP\t8\n",
    ",\t,\t2\n",
    "61\tCD\t5\n",
    "years\tNNS\t6\n",
    "old\tJJ\t2\n",
    ",\t,\t2\n",
    "will\tMD\t0\n",
    "join\tVB\t8\n",
    "the\tDT\t11\n",
    "board\tNN\t9\n",
    "as\tIN\t9\n",
    "a\tDT\t15\n",
    "nonexecutive\tJJ\t15\n",
    "director\tNN\t12\n",
    "Nov.\tNNP\t9\n",
    "29\tCD\t16\n",
    ".\t.\t8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Splits\n",
    "\n",
    "The corpus contains 200 documents.\n",
    "\n",
    "   * **Train**: Documents 1-100\n",
    "   * **Validation**: Documents 101-150\n",
    "   * **Test**: Documents 151-199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Download** the corpus.\n",
    "* **Encode** the corpus into a pandas.DataFrame object.\n",
    "* **Split** it in training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of Vocabulary (OOV) words in training set\n",
    "We see words in the training set that are not alredy embedded through Glove (50) model, in addition we define the set oov_terms with all those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file management\n",
    "import sys\n",
    "import shutil\n",
    "import urllib\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# dataframe management\n",
    "import pandas as pd\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "\n",
    "# for readability\n",
    "from typing import Iterable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "        \n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    \n",
    "    with zipfile.ZipFile(download_path) as loaded_tar:\n",
    "        loaded_tar.extractall(path=extract_path, pwd=None)\n",
    "    print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: c:\\Users\\Utente\\Desktop\\UNIVERSITA'\\AI\\2 Anno\\Natural Language Processing\\_ Esame\\Assignment 1\\NLP_POS-tagging\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_tar_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_tar_path.exists():\n",
    "    download_dataset(dataset_tar_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_tar_path, dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode and Split\n",
    "\n",
    "The aim of the code below is to find a way to create a dataframe starting from all the files downloaded before.\n",
    "For every downloaded file, we check the number through the function find_number(), we decide if it belongs to train, validation or test given that number, we then split it into rows to get the word and the POS and to check where a phrase ends. Given all this informations we can create a list whose columns are: \n",
    "1. num_file: the number of the file\n",
    "2. phrase_id: the id of the phrase contained in a file \n",
    "3. text: the text that has to be analyzed\n",
    "4. pos: the tag assigned to the text\n",
    "5. split: the split to which the text belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_number(string):\n",
    "    \"\"\"\n",
    "    This function finds the number written in a string.\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\d+', string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_rows = []\n",
    "id = 0\n",
    "\n",
    "folder = dataset_folder.joinpath(dataset_name)\n",
    "for file_path in folder.glob('*.dp'):\n",
    "    num_file = int(find_number(file_path.name)[0])\n",
    "    id = 1\n",
    "    \n",
    "    with file_path.open(mode='r', encoding='utf-8') as text_file:\n",
    "        \n",
    "        if num_file < 101:\n",
    "            split = \"train\"\n",
    "        elif num_file >= 101 and num_file < 151:\n",
    "            split = \"validation\"\n",
    "        else:\n",
    "            split = \"test\"\n",
    "\n",
    "        for row in text_file.readlines():\n",
    "            if row=='\\n' or row=='':\n",
    "                id += 1\n",
    "\n",
    "            else:\n",
    "                text, pos, _ = row.split('\\t')\n",
    "\n",
    "                dataframe_row = {\n",
    "                    \"num_file\": num_file,\n",
    "                    \"phrase_id\": str(num_file) + \"_\" + str(id),\n",
    "                    \"text\": text,\n",
    "                    \"pos\": pos,\n",
    "                    \"split\": split\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_file</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>old</td>\n",
       "      <td>JJ</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>will</td>\n",
       "      <td>MD</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>join</td>\n",
       "      <td>VB</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>board</td>\n",
       "      <td>NN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>as</td>\n",
       "      <td>IN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>nonexecutive</td>\n",
       "      <td>JJ</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>director</td>\n",
       "      <td>NN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>Nov.</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>29</td>\n",
       "      <td>CD</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1_2</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1_2</td>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_file phrase_id          text  pos  split\n",
       "0          1       1_1        Pierre  NNP  train\n",
       "1          1       1_1        Vinken  NNP  train\n",
       "2          1       1_1             ,    ,  train\n",
       "3          1       1_1            61   CD  train\n",
       "4          1       1_1         years  NNS  train\n",
       "5          1       1_1           old   JJ  train\n",
       "6          1       1_1             ,    ,  train\n",
       "7          1       1_1          will   MD  train\n",
       "8          1       1_1          join   VB  train\n",
       "9          1       1_1           the   DT  train\n",
       "10         1       1_1         board   NN  train\n",
       "11         1       1_1            as   IN  train\n",
       "12         1       1_1             a   DT  train\n",
       "13         1       1_1  nonexecutive   JJ  train\n",
       "14         1       1_1      director   NN  train\n",
       "15         1       1_1          Nov.  NNP  train\n",
       "16         1       1_1            29   CD  train\n",
       "17         1       1_1             .    .  train\n",
       "18         1       1_2           Mr.  NNP  train\n",
       "19         1       1_2        Vinken  NNP  train"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataframe_rows)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 2 - 0.5 points] Text encoding\n",
    "\n",
    "To train a neural POS tagger, you first need to encode text into numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typing\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text pre-processing\n",
    "In the code below we pre-processed the df dataframe in order to reduce the number of different words. Our text pre-processing consist just in lowering the text of words. <br>\n",
    "**NB: should we add somenthing to the pre processing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms given text to lower case.\n",
    "    \"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_PIPELINE = [\n",
    "                          lower\n",
    "                          ]\n",
    "\n",
    "def text_prepare(text: str,\n",
    "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing text...\n",
      "\n",
      "[Debug] Before:\n",
      "Pierre\n",
      "\n",
      "[Debug] After:\n",
      "pierre\n",
      "\n",
      "Pre-processing completed!\n"
     ]
    }
   ],
   "source": [
    "print('Pre-processing text...')\n",
    "\n",
    "print()\n",
    "print(f'[Debug] Before:\\n{df.text.values[0]}')\n",
    "print()\n",
    "\n",
    "# Replace each sentence with its pre-processed version\n",
    "df['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n",
    "\n",
    "print(f'[Debug] After:\\n{df.text.values[0]}')\n",
    "print()\n",
    "\n",
    "print(\"Pre-processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary creation for training set\n",
    "We define a vocabulary for the training set assigning to each word a random index, the building_vocabulary function returns a list containing:<br>\n",
    "- word vocabulary: vocabulary index to word\n",
    "- inverse word vocabulary: word to vocabulary index\n",
    "- word listing: set of unique terms that build up the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['split']=='train']\n",
    "df_val = df[df['split']=='validation']\n",
    "df_test = df[df['split']=='test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe embeddings (50)\n",
    "Download GloVe 50 embedding where most of the words are alredy embedded in an embedding model that associate each word to a vector of dimension 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(model_type: str,\n",
    "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    download_path = \"\"\n",
    "\n",
    "    if model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available one: glove\")\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "embedding_model = load_embedding_model(model_type=\"glove\",\n",
    "                                       embedding_dimension=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (400001, 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {} # word to idx\n",
    "embedding_matrix_glove = np.zeros((400001, embedding_dim))\n",
    "\n",
    "for i in range(0, 400000):\n",
    "    vocab[embedding_model.index_to_key[i]] = i+1\n",
    "    embedding_matrix_glove[i+1] = embedding_model.vectors[i]\n",
    "\n",
    "print(f'Embedding matrix shape: {embedding_matrix_glove.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing: List[str]):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 359 (4.85%)\n"
     ]
    }
   ],
   "source": [
    "word_listing = set(df_train['text'])\n",
    "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
    "oov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\n",
    "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add the OOV in the train set to the vocabulary and the embedded matrix </br>\n",
    "*NB: maybe is better to define the OOV as the mean of all the other word, link for report (https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt)* <br>\n",
    "Our embedding matrix has the following columns:\n",
    "- column 0 is all zeros, represents the embedding vector for padding\n",
    "- columns 1 to 400001 are the embedding vectors for the words in GloVe\n",
    "- columns 400002 to 400360 are the embedding vectors for the words OOV in the training set (random vector)\n",
    "- column 400361 is the embedding vector for the words OOV in the final vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400361\n",
      "New embedding matrix size: (400361, 50)\n"
     ]
    }
   ],
   "source": [
    "for word in oov_terms:\n",
    "    vocab[word] = 400002 + oov_terms.index(word)\n",
    "    embedding_matrix_glove = np.append(embedding_matrix_glove, np.random.uniform(-0.25, 0.25, 50).reshape(1, 50), axis=0)\n",
    "\n",
    "vocab['[OOV]'] = len(vocab) + 1\n",
    "average_oov = np.mean(embedding_matrix_glove, axis=0)\n",
    "embedding_matrix = np.append(embedding_matrix_glove, average_oov.reshape(1, 50), axis=0)\n",
    "\n",
    "vocab['[PAD]'] = 0\n",
    "\n",
    "print(len(vocab))\n",
    "print(f\"New embedding matrix size: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding for training set\n",
    "We create the embedding matrix for all the training set:\n",
    "- using GloVe embeddings for alredy known words\n",
    "- assigning to each OOV word a random value.\n",
    "\n",
    "**NB: maybe instead of random we can define OOV with the mean of its neighbour word embeddings (tutorial 2)** <br>\n",
    "**NB: we can even add all embedding in GloVe to the embedding matrix, even if they are not in train set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 1.0 points] Model definition\n",
    "\n",
    "You are now tasked to define your neural POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
    "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
    "\n",
    "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
    "\n",
    "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pos = list(set(df_train['pos']))\n",
    "\n",
    "def pos_to_int(string):\n",
    "    length = len(list_of_pos)\n",
    "    for i in range(length):\n",
    "        if list_of_pos[i]==string:\n",
    "            return [1 if j == i else 0 for j in range(length)]\n",
    "\n",
    "def int_to_pos(pred):\n",
    "    idx_hot_encoding, _ = max(enumerate(pred), key=lambda x: x[1])\n",
    "    return list_of_pos[idx_hot_encoding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(pos_to_int('JJ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline \n",
    "**NB: reference slides 08 pag 38** <br>\n",
    "https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GloVe_dim = 50 # GloVe embedding\n",
    "units_bi = 100\n",
    "\n",
    "n_unique_words = len(vocab) # input and output layer\n",
    "outputs_dim = len(set(df_train['pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"baseline\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 50)          20018050  \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, None, 200)         120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 45)          9045      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20147895 (76.86 MB)\n",
      "Trainable params: 129845 (507.21 KB)\n",
      "Non-trainable params: 20018050 (76.36 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline = tf.keras.Sequential(name='baseline')\n",
    "\n",
    "baseline.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=True, trainable=False))\n",
    "baseline.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
    "baseline.add(layers.Dense(outputs_dim, activation='softmax'))\n",
    "\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 50)          20018050  \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, None, 200)         120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, None, 200)         240800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, None, 45)          9045      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20388695 (77.78 MB)\n",
      "Trainable params: 370645 (1.41 MB)\n",
      "Non-trainable params: 20018050 (76.36 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = tf.keras.Sequential(name='Model_1')\n",
    "\n",
    "model_1.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=True, trainable=False))\n",
    "model_1.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
    "model_1.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
    "model_1.add(layers.Dense(outputs_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_dense = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, None, 50)          20018050  \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, None, 200)         120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, None, 100)         20100     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, None, 45)          4545      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20163495 (76.92 MB)\n",
      "Trainable params: 145445 (568.14 KB)\n",
      "Non-trainable params: 20018050 (76.36 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = tf.keras.Sequential(name='Model_2')\n",
    "\n",
    "model_2.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=True, trainable=False))\n",
    "model_2.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
    "model_2.add(layers.Dense(units_dense, activation='softmax'))\n",
    "model_2.add(layers.Dense(outputs_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try models\n",
    "\n",
    "https://medium.com/analytics-vidhya/author-multi-class-text-classification-using-bidirectional-lstm-keras-c9a533a1cc4a \n",
    "an example of a bi LSTM implementation with padding <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_idx(word):\n",
    "    try:\n",
    "        idx = vocab[word]\n",
    "    except(KeyError):\n",
    "        idx = vocab['[OOV]']\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reduction = df_train.loc[df_train['num_file'] <= 20]\n",
    "df_val_reduction = df_val.loc[df_val['num_file'] <= 110]\n",
    "df_test_reduction = df_test.loc[df_test['num_file'] <= 160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [ [assign_idx(word) for word in df_train_reduction[df_train_reduction['phrase_id']==nr_phrase]['text']] for nr_phrase in set(df_train_reduction['phrase_id']) ]\n",
    "x_val   = [ [assign_idx(word) for word in df_val_reduction[df_val_reduction['phrase_id']==nr_phrase]['text']] for nr_phrase in set(df_val_reduction['phrase_id']) ]\n",
    "x_test   = [ [assign_idx(word) for word in df_test_reduction[df_test_reduction['phrase_id']==nr_phrase]['text']] for nr_phrase in set(df_test_reduction['phrase_id']) ]\n",
    "\n",
    "y_train = [ [pos_to_int(pos) for pos in df_train_reduction[df_train_reduction['phrase_id']==nr_phrase]['pos']] for nr_phrase in set(df_train_reduction['phrase_id']) ]\n",
    "y_val   = [ [pos_to_int(pos) for pos in df_val_reduction[df_val_reduction['phrase_id']==nr_phrase]['pos']] for nr_phrase in set(df_val_reduction['phrase_id']) ]\n",
    "y_test   = [ [pos_to_int(pos) for pos in df_test_reduction[df_test_reduction['phrase_id']==nr_phrase]['pos']] for nr_phrase in set(df_test_reduction['phrase_id']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "#In order to put the data data in data in the model we need to pad the array representing the words / pos\n",
    "\n",
    "pad = max(max([len(phrase) for phrase in x_train]), max([len(phrase) for phrase in x_val]), max([len(phrase) for phrase in x_test]))\n",
    "print(pad)\n",
    "\n",
    "x_train_pad = [phrase + np.zeros((pad-len(phrase),)).tolist() for phrase in x_train]\n",
    "x_val_pad = [phrase + np.zeros((pad-len(phrase),)).tolist() for phrase in x_val]\n",
    "x_test_pad = [phrase + np.zeros((pad-len(phrase),)).tolist() for phrase in x_test]\n",
    "\n",
    "y_train_pad = [phrase + np.zeros((pad-len(phrase), 45)).tolist() for phrase in y_train]\n",
    "y_val_pad = [phrase + np.zeros((pad-len(phrase), 45)).tolist() for phrase in y_val]\n",
    "y_test_pad = [phrase + np.zeros((pad-len(phrase), 45)).tolist() for phrase in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 1.0 points] Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
    "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively) \n",
    "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute metrics on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Pick the **best** performing model according to the observed validation set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: todo fine tuning on embedding layer https://stackoverflow.com/questions/40345607/how-does-fine-tuning-word-embeddings-work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "30/30 - 9s - loss: 3.7153 - accuracy: 0.0350 - f1_m: 0.0000e+00 - val_loss: 3.7222 - val_accuracy: 0.0370 - val_f1_m: 0.0000e+00 - 9s/epoch - 293ms/step\n",
      "Epoch 2/2\n",
      "30/30 - 2s - loss: 3.7130 - accuracy: 0.0369 - f1_m: 0.0000e+00 - val_loss: 3.7200 - val_accuracy: 0.0386 - val_f1_m: 0.0000e+00 - 2s/epoch - 57ms/step\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7192 - accuracy: 0.0362 - f1_m: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model = baseline\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy', f1_m], optimizer='Adadelta')\n",
    "history = model.fit(x_train_pad, y_train_pad, batch_size=8, epochs=2, validation_data=(x_val_pad, y_val_pad), verbose=2)\n",
    "loss, accuracy, f1_score = model.evaluate(x_test_pad, y_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 22ms/step\n",
      "IN\n",
      "RBS\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_val_pad)\n",
    "print(int_to_pos(y_val_pad[0][2]))\n",
    "print(int_to_pos(pred[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def graph_plots(history, string):\n",
    "#   plt.plot(history.history[string])\n",
    "#   plt.plot(history.history['val_'+string])\n",
    "#   plt.xlabel(\"Epochs\")\n",
    "#   plt.ylabel(string)\n",
    "#   plt.legend([string, 'val_'+string])\n",
    "#   plt.show()\n",
    "  \n",
    "# graph_plots(history, \"accuracy\")\n",
    "# graph_plots(history, \"loss\")\n",
    "# graph_plots(history, \"f1_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Error Analysis\n",
    "\n",
    "You are tasked to evaluate your best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Compare the errors made on the validation and test sets.\n",
    "* Aggregate model errors into categories (if possible) \n",
    "* Comment the about errors and propose possible solutions on how to address them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 7 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trainable Embeddings\n",
    "\n",
    "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Keras TimeDistributed Dense layer\n",
    "\n",
    "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Model performance on most/less frequent classes.\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Punctuation\n",
    "\n",
    "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
    "\n",
    "You should **ignore** it during metrics computation.\n",
    "\n",
    "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
