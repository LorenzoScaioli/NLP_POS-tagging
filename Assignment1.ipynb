{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: POS tagging, Sequence labelling, RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "You are tasked to address the task of POS tagging.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/pos_tagging.png\" alt=\"POS tagging\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 1 - 0.5 points] Corpus\n",
    "\n",
    "You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n",
    "\n",
    "**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n",
    "\n",
    "### Example\n",
    "\n",
    "```Pierre\tNNP\t2\n",
    "Vinken\tNNP\t8\n",
    ",\t,\t2\n",
    "61\tCD\t5\n",
    "years\tNNS\t6\n",
    "old\tJJ\t2\n",
    ",\t,\t2\n",
    "will\tMD\t0\n",
    "join\tVB\t8\n",
    "the\tDT\t11\n",
    "board\tNN\t9\n",
    "as\tIN\t9\n",
    "a\tDT\t15\n",
    "nonexecutive\tJJ\t15\n",
    "director\tNN\t12\n",
    "Nov.\tNNP\t9\n",
    "29\tCD\t16\n",
    ".\t.\t8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Splits\n",
    "\n",
    "The corpus contains 200 documents.\n",
    "\n",
    "   * **Train**: Documents 1-100\n",
    "   * **Validation**: Documents 101-150\n",
    "   * **Test**: Documents 151-199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Download** the corpus.\n",
    "* **Encode** the corpus into a pandas.DataFrame object.\n",
    "* **Split** it in training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of Vocabulary (OOV) words in training set\n",
    "We see words in the training set that are not alredy embedded through Glove (50) model, in addition we define the set oov_terms with all those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file management\n",
    "import sys\n",
    "import shutil\n",
    "import urllib\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# dataframe management\n",
    "import pandas as pd\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "\n",
    "# for readability\n",
    "from typing import Iterable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "        \n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    \n",
    "    with zipfile.ZipFile(download_path) as loaded_tar:\n",
    "        loaded_tar.extractall(path=extract_path, pwd=None)\n",
    "    print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: /Users/giacomopiergentili/Developer/NLP/Assignment1/NLP_POS-tagging\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_tar_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_tar_path.exists():\n",
    "    download_dataset(dataset_tar_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_tar_path, dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode and Split\n",
    "\n",
    "The aim of the code below is to find a way to create a dataframe starting from all the files downloaded before.\n",
    "For every downloaded file, we check the number through the function find_number(), we decide if it belongs to train, validation or test given that number, we then split it into rows to get the word and the POS and to check where a phrase ends. Given all this informations we can create a list whose columns are: \n",
    "1. num_file: the number of the file\n",
    "2. phrase_id: the id of the phrase contained in a file \n",
    "3. text: the text that has to be analyzed\n",
    "4. pos: the tag assigned to the text\n",
    "5. split: the split to which the text belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_number(string):\n",
    "    \"\"\"\n",
    "    This function finds the number written in a string.\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\d+', string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_rows = []\n",
    "id = 0\n",
    "\n",
    "folder = dataset_folder.joinpath(dataset_name)\n",
    "for file_path in folder.glob('*.dp'):\n",
    "    num_file = int(find_number(file_path.name)[0])\n",
    "    id = 1\n",
    "    \n",
    "    with file_path.open(mode='r', encoding='utf-8') as text_file:\n",
    "        \n",
    "        if num_file < 101:\n",
    "            split = \"train\"\n",
    "        elif num_file >= 101 and num_file < 151:\n",
    "            split = \"validation\"\n",
    "        else:\n",
    "            split = \"test\"\n",
    "\n",
    "        for row in text_file.readlines():\n",
    "            if row=='\\n' or row=='':\n",
    "                id += 1\n",
    "\n",
    "            else:\n",
    "                text, pos, _ = row.split('\\t')\n",
    "\n",
    "                dataframe_row = {\n",
    "                    \"num_file\": num_file,\n",
    "                    \"phrase_id\": str(num_file) + \"_\" + str(id),\n",
    "                    \"text\": text,\n",
    "                    \"pos\": pos,\n",
    "                    \"split\": split\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_file</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>95_1</td>\n",
       "      <td>In</td>\n",
       "      <td>IN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>95_1</td>\n",
       "      <td>reference</td>\n",
       "      <td>NN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>95_1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95</td>\n",
       "      <td>95_1</td>\n",
       "      <td>your</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>95_1</td>\n",
       "      <td>Oct.</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_file phrase_id       text   pos  split\n",
       "0        95      95_1         In    IN  train\n",
       "1        95      95_1  reference    NN  train\n",
       "2        95      95_1         to    TO  train\n",
       "3        95      95_1       your  PRP$  train\n",
       "4        95      95_1       Oct.   NNP  train"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataframe_rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 2 - 0.5 points] Text encoding\n",
    "\n",
    "To train a neural POS tagger, you first need to encode text into numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typing\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text pre-processing\n",
    "In the code below we pre-processed the df dataframe in order to reduce the number of different words. Our text pre-processing consist just in lowering the text of words. <br>\n",
    "**NB: should we add somenthing to the pre processing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms given text to lower case.\n",
    "    \"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_PIPELINE = [\n",
    "                          lower\n",
    "                          ]\n",
    "\n",
    "def text_prepare(text: str,\n",
    "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing text...\n",
      "\n",
      "[Debug] Before:\n",
      "In\n",
      "\n",
      "[Debug] After:\n",
      "in\n",
      "\n",
      "Pre-processing completed!\n"
     ]
    }
   ],
   "source": [
    "print('Pre-processing text...')\n",
    "\n",
    "print()\n",
    "print(f'[Debug] Before:\\n{df.text.values[0]}')\n",
    "print()\n",
    "\n",
    "# Replace each sentence with its pre-processed version\n",
    "df['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n",
    "\n",
    "print(f'[Debug] After:\\n{df.text.values[0]}')\n",
    "print()\n",
    "\n",
    "print(\"Pre-processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary creation for training set\n",
    "We define a vocabulary for the training set assigning to each word a random index, the building_vocabulary function returns a list containing:<br>\n",
    "- word vocabulary: vocabulary index to word\n",
    "- inverse word vocabulary: word to vocabulary index\n",
    "- word listing: set of unique terms that build up the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['split']=='train']\n",
    "df_val = df[df['split']=='validation']\n",
    "df_test = df[df['split']=='test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe embeddings (50)\n",
    "Download GloVe 50 embedding where most of the words are alredy embedded in an embedding model that associate each word to a vector of dimension 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(model_type: str,\n",
    "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    download_path = \"\"\n",
    "\n",
    "    if model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available one: glove\")\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "embedding_model = load_embedding_model(model_type=\"glove\",\n",
    "                                       embedding_dimension=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (400000, 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {} # word to idx\n",
    "embedding_matrix_glove = np.zeros((400000, embedding_dim))\n",
    "\n",
    "for i in range(0, 400000):\n",
    "    vocab[embedding_model.index_to_key[i]] = i\n",
    "    embedding_matrix_glove[i] = embedding_model.vectors[i]\n",
    "\n",
    "print(f'Embedding matrix shape: {embedding_matrix_glove.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing: List[str]):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 359 (4.85%)\n"
     ]
    }
   ],
   "source": [
    "word_listing = set(df_train['text'])\n",
    "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
    "oov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\n",
    "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add the OOV in the train set to the vocabulary and the embedded matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400360\n",
      "New embedding matrix size: (400360, 50)\n"
     ]
    }
   ],
   "source": [
    "for word in oov_terms:\n",
    "    vocab[word] = 400001 + oov_terms.index(word)\n",
    "    embedding_matrix_glove = np.append(embedding_matrix_glove, np.random.uniform(-0.25, 0.25, 50).reshape(1, 50), axis=0)\n",
    "\n",
    "vocab['<unk>'] = len(vocab) + 1\n",
    "embedding_matrix = np.append(embedding_matrix_glove, np.zeros(50).reshape(1, 50), axis=0)\n",
    "\n",
    "print(len(vocab))\n",
    "print(f\"New embedding matrix size: {embedding_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding for training set\n",
    "We create the embedding matrix for all the training set:\n",
    "- using GloVe embeddings for alredy known words\n",
    "- assigning to each OOV word a random value.\n",
    "\n",
    "**NB: maybe instead of random we can define OOV with the mean of its neighbour word embeddings (tutorial 2)** <br>\n",
    "**NB: we can even add all embedding in GloVe to the embedding matrix, even if they are not in train set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 1.0 points] Model definition\n",
    "\n",
    "You are now tasked to define your neural POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
    "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
    "\n",
    "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
    "\n",
    "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_to_int(string):\n",
    "    length = len(set(df_train['pos']))\n",
    "    for i in range(length):\n",
    "        if list(set(df_train['pos']))[i]==string:\n",
    "            return np.array([1 if j == i else 0 for j in range(length)])\n",
    "        \n",
    "# len(set(df_train['pos'])) == len(set(df['pos']))\n",
    "# set(df_train['pos'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(pos_to_int('JJ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline \n",
    "**NB: reference slides 08 pag 38** <br>\n",
    "https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GloVe_dim = 50 # GloVe embedding\n",
    "units_bi = 100\n",
    "n_unique_words = len(word_to_idx) # input and output layer\n",
    "outputs_dim = len(set(df_train['pos']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Eager execution of tf.constant with unsupported shape. Tensor [[ 0.418       0.24968    -0.41242    ... -0.18411    -0.11514\n  -0.78581   ]\n [ 0.013441    0.23682    -0.16899    ... -0.56657     0.044691\n   0.30392   ]\n [ 0.15164     0.30177    -0.16763    ... -0.35652     0.016413\n   0.10216   ]\n ...\n [ 0.03095084 -0.20284662 -0.00804097 ...  0.02355134 -0.13017274\n  -0.21139628]\n [ 0.18244837 -0.03331343  0.16502209 ... -0.15402314 -0.12217832\n  -0.23983172]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]] (converted from [[ 0.41800001  0.24968    -0.41242    ... -0.18411    -0.11514\n  -0.78580999]\n [ 0.013441    0.23682    -0.16899    ... -0.56656998  0.044691\n   0.30392   ]\n [ 0.15164     0.30177    -0.16763    ... -0.35652     0.016413\n   0.10216   ]\n ...\n [ 0.03095084 -0.20284662 -0.00804097 ...  0.02355135 -0.13017275\n  -0.21139627]\n [ 0.18244838 -0.03331343  0.16502209 ... -0.15402314 -0.12217831\n  -0.23983171]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]]) has 20018000 elements, but got `shape` (7404, 50) with 370200 elements).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/giacomopiergentili/Developer/NLP/Assignment1/NLP_POS-tagging/Assignment1.ipynb Cell 44\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giacomopiergentili/Developer/NLP/Assignment1/NLP_POS-tagging/Assignment1.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m baseline \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbaseline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/giacomopiergentili/Developer/NLP/Assignment1/NLP_POS-tagging/Assignment1.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m baseline\u001b[39m.\u001b[39;49madd(layers\u001b[39m.\u001b[39;49mEmbedding(n_unique_words, GloVe_dim, embeddings_initializer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49minitializers\u001b[39m.\u001b[39;49mConstant(embedding_matrix))) \u001b[39m#trainable=False\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giacomopiergentili/Developer/NLP/Assignment1/NLP_POS-tagging/Assignment1.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m baseline\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mBidirectional(layers\u001b[39m.\u001b[39mLSTM(units_bi, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giacomopiergentili/Developer/NLP/Assignment1/NLP_POS-tagging/Assignment1.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m baseline\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDense(outputs_dim, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp-env/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp-env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp-env/lib/python3.11/site-packages/keras/src/initializers/initializers.py:265\u001b[0m, in \u001b[0;36mConstant.__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m layout:\n\u001b[1;32m    262\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mcall_with_layout(\n\u001b[1;32m    263\u001b[0m         tf\u001b[39m.\u001b[39mconstant, layout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue, shape\u001b[39m=\u001b[39mshape, dtype\u001b[39m=\u001b[39mdtype\n\u001b[1;32m    264\u001b[0m     )\n\u001b[0;32m--> 265\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mconstant(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue, dtype\u001b[39m=\u001b[39;49m_get_dtype(dtype), shape\u001b[39m=\u001b[39;49mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: Eager execution of tf.constant with unsupported shape. Tensor [[ 0.418       0.24968    -0.41242    ... -0.18411    -0.11514\n  -0.78581   ]\n [ 0.013441    0.23682    -0.16899    ... -0.56657     0.044691\n   0.30392   ]\n [ 0.15164     0.30177    -0.16763    ... -0.35652     0.016413\n   0.10216   ]\n ...\n [ 0.03095084 -0.20284662 -0.00804097 ...  0.02355134 -0.13017274\n  -0.21139628]\n [ 0.18244837 -0.03331343  0.16502209 ... -0.15402314 -0.12217832\n  -0.23983172]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]] (converted from [[ 0.41800001  0.24968    -0.41242    ... -0.18411    -0.11514\n  -0.78580999]\n [ 0.013441    0.23682    -0.16899    ... -0.56656998  0.044691\n   0.30392   ]\n [ 0.15164     0.30177    -0.16763    ... -0.35652     0.016413\n   0.10216   ]\n ...\n [ 0.03095084 -0.20284662 -0.00804097 ...  0.02355135 -0.13017275\n  -0.21139627]\n [ 0.18244838 -0.03331343  0.16502209 ... -0.15402314 -0.12217831\n  -0.23983171]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]]) has 20018000 elements, but got `shape` (7404, 50) with 370200 elements)."
     ]
    }
   ],
   "source": [
    "baseline = tf.keras.Sequential(name='baseline')\n",
    "\n",
    "baseline.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix))) #trainable=False\n",
    "baseline.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
    "baseline.add(layers.Dense(outputs_dim, activation='softmax'))\n",
    "\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 50)          370200    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, None, 200)         120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, None, 200)         240800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 45)          9045      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 740845 (2.83 MB)\n",
      "Trainable params: 740845 (2.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = tf.keras.Sequential(name='Model_1')\n",
    "\n",
    "model_1.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)))\n",
    "model_1.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
    "model_1.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
    "model_1.add(layers.Dense(outputs_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_dense = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 50)          370200    \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, None, 200)         120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 100)         20100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, None, 45)          4545      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 515645 (1.97 MB)\n",
      "Trainable params: 515645 (1.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = tf.keras.Sequential(name='Model_2')\n",
    "\n",
    "model_2.add(layers.Embedding(n_unique_words, GloVe_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)))\n",
    "model_2.add(layers.Bidirectional(layers.LSTM(units_bi, activation='relu', return_sequences=True)))\n",
    "model_2.add(layers.Dense(units_dense, activation='softmax'))\n",
    "model_2.add(layers.Dense(outputs_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try models\n",
    "\n",
    "https://medium.com/analytics-vidhya/author-multi-class-text-classification-using-bidirectional-lstm-keras-c9a533a1cc4a \n",
    "an example of a bi LSTM implementation with padding <br>\n",
    "Things to do:\n",
    "- add to the embedding matrix a vector for OOV words \n",
    "- create a way so that if a word is not in the vocabulary is categorized as OOV and the related index\n",
    "- put all the GloVe vocabulary in teh embedding matrix (to avoid the case that a word in the test set is OOV even if was in GloVe, but not in Train)\n",
    "- create a PAD index  and vector in the embedding matrix so that the nr of the index related to PAD is the const_pad\n",
    "- create a PAD value in the pos output so that in the result all padding are categorized as it, actually is all zero vector, maybe is alredy good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a test with train and val both from train set (x_train from file nr 1 and x_val from file nr 2)\n",
    "\n",
    "x_train = [ [word_to_idx[word] for word in df_train[df_train['phrase_id']==nr_phrase]['text']] for nr_phrase in set(df_train[df_train['num_file']==1]['phrase_id']) ]\n",
    "x_val   = [ [word_to_idx[word] for word in df_train[df_train['phrase_id']==nr_phrase]['text']] for nr_phrase in set(df_train[df_train['num_file']==2]['phrase_id']) ]\n",
    "\n",
    "y_train = [ np.array([pos_to_int(el) for el in df_train[df_train['phrase_id']==nr_phrase]['pos']]) for nr_phrase in set(df_train[df_train['num_file']==1]['phrase_id']) ]\n",
    "y_val   = [ np.array([pos_to_int(el) for el in df_train[df_train['phrase_id']==nr_phrase]['pos']]) for nr_phrase in set(df_train[df_train['num_file']==2]['phrase_id']) ]\n",
    "\n",
    "#In order to put the data data in data in the model we need to pad the array representing the words / pos\n",
    "\n",
    "padd = 30\n",
    "const_pad = 0\n",
    "# const_pad_y = np.zeros(45) At the end we will concatenate the array\n",
    "\n",
    "y_train_pad = np.array([np.concatenate((batch, np.zeros((padd-len(batch),45))), axis=0) for batch in y_train])\n",
    "y_val_pad = np.array([np.concatenate((batch, np.zeros((padd-len(batch),45))), axis=0) for batch in y_val])\n",
    "\n",
    "x_train_pad = np.array([np.pad(np.array(batch), (const_pad, padd-len(batch))) for batch in x_train])\n",
    "x_val_pad = np.array([np.pad(np.array(batch), (const_pad, padd-len(batch))) for batch in x_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 30\n",
      "True 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(x_train_pad)):\n",
    "    print(len(x_train_pad[i])==len(y_train_pad[i]), len(x_train_pad[i]))\n",
    "\n",
    "# [np.concatenate((batch, np.zeros((padd-len(batch),45))), axis=0) for batch in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9953 - val_loss: 3.3079\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.9952 - val_loss: 3.3078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2985a5190>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = baseline\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adadelta')\n",
    "model.fit(x_train_pad, y_train_pad, epochs=2, validation_data=(x_val_pad, y_val_pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 1.0 points] Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
    "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively) \n",
    "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute metrics on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Pick the **best** performing model according to the observed validation set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Error Analysis\n",
    "\n",
    "You are tasked to evaluate your best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Compare the errors made on the validation and test sets.\n",
    "* Aggregate model errors into categories (if possible) \n",
    "* Comment the about errors and propose possible solutions on how to address them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 7 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trainable Embeddings\n",
    "\n",
    "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Keras TimeDistributed Dense layer\n",
    "\n",
    "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Model performance on most/less frequent classes.\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Punctuation\n",
    "\n",
    "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
    "\n",
    "You should **ignore** it during metrics computation.\n",
    "\n",
    "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
